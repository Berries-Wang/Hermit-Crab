10.Kubernetes服务发现

Kubernetes服务发现
==============

Kubernetes 支持两种主要的服务发现模式：

1.  环境变量
2.  DNS

服务发现方式: 环境变量
------------

kubelet 查找有效的 Service，并针对每一个 Service，向其所在节点上的 Pod 注入一组环境变量。支持的环境变量有：

*   Docker links 兼容 的环境变量
*   {SVCNAME}\_SERVICE\_HOST 和 {SVCNAME}\_SERVICE\_PORT
    *   Service name 被转换为大写
    *   小数点 . 被转换为下划线 \_
    *   例如，Service redis-master 暴露 TCP 端口 6379，其 Cluster IP 为 10.0.0.11，对应的环境变量如下所示：

                REDIS_MASTER_SERVICE_HOST=10.0.0.11
                REDIS_MASTER_SERVICE_PORT=6379
                REDIS_MASTER_PORT=tcp://10.0.0.11:6379
                REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379
                REDIS_MASTER_PORT_6379_TCP_PROTO=tcp
                REDIS_MASTER_PORT_6379_TCP_PORT=6379
                REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11
    

* * *

服务发现方式：DNS
----------

*   CoreDNS 监听 Kubernetes API 上创建和删除 Service 的事件，并为每一个 Service 创建一条 DNS 记录。集群中所有的 Pod 都可以使用 DNS Name 解析到 Service 的 IP 地址。
*   例如，名称空间 my-ns 中的 Service my-service，将对应一条 DNS 记录 my-service.my-ns(**格式:${service\_name.namespace\_name}**)。 名称空间 my-ns 中的Pod可以直接 nslookup my-service （my-service.my-ns 也可以）。其他名称空间的 Pod 必须使用 my-service.my-ns。my-service 和 my-service.my-ns 都将被解析到 Service 的 Cluster IP。
*   Kubernetes 同样支持 DNS SRV（Service）记录，用于查找一个命名的端口。假设 my-service.my-ns Service 有一个 TCP 端口名为 http，则，您可以 nslookup \_http.\_tcp.my-service.my-ns 以发现该Service 的 IP 地址及端口 http
*   对于 ExternalName 类型的 Service，只能通过 DNS 的方式进行服务发现

### 虚拟IP的实现

#### 避免冲突

*   Kubernetes 的一个设计哲学是：尽量避免非人为错误产生的可能性。就设计 Service 而言，Kubernetes 应该将您选择的端口号与其他人选择的端口号隔离开。为此，Kubernetes 为每一个 Service 分配一个该 Service 专属的 IP 地址。
*   为了确保每个 Service 都有一个唯一的 IP 地址，kubernetes 在创建 Service 之前，先更新 etcd 中的一个全局分配表，如果更新失败（例如 IP 地址已被其他 Service 占用），则 Service 不能成功创建。
*   Kubernetes 使用一个后台控制器检查该全局分配表中的 IP 地址的分配是否仍然有效，并且自动清理不再被 Service 使用的 IP 地址。

#### Service的IP地址

Pod的IP地址路由到一个明确的目标，然而Service的IP地址则不同，通常背后并不对应一个唯一的目标。**kube-proxy使用iptables(Linux中的报文处理逻辑)来定义虚拟IP地址**。当客户端连接到该虚拟IP地址的时候，他们的网络请求将自动发送到一个合适的Endpoint。Service对应的环境变量和DNS实际上反应的是Service的虚拟IP和端口

*   什么是EndPoint
    *   endpoint是k8s集群中的一个资源对象，存储在etcd中，用来记录一个service对应的所有pod的访问地址。

##### 虚拟IP的实现方式之Userspace

*   ![](10.Kubernetes%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0.resources/699647CC-2347-49D3-8A07-0E2B8D02BAA9.png)
    1.  kube-proxy 监听 kubernetes master 以获得添加和移除 Service / Endpoint 的事件
    2.  kube-proxy 在其所在的节点（每个节点都有 kube-proxy）上为每一个 Service 打开一个随机端口
    3.  kube-proxy 安装 iptables 规则，将发送到该 Service 的 ClusterIP（虚拟 IP）/ Port 的请求重定向到该随机端口
    4.  任何发送到该随机端口的请求将被代理转发到该 Service 的后端 Pod 上（kube-proxy 从 Endpoint 信息中获得可用 Pod）
    5.  kube-proxy 在决定将请求转发到后端哪一个 Pod 时，默认使用 round-robin（轮询）算法，并会考虑到 Service 中的 SessionAffinity 的设定
*   当后端Service被创建的时候，Kubernetes master为其分配一个虚拟IP地址(假设为10.0.0.1),并假设Service的端口是1234.集群中所有的Kube-proxy都实时监听Service的创建和删除。Service创建后，kube-proxy将打开一个新的随机端口，并设定iptables的转发规则(以便将该Service虚拟IP的网络请求全部转发到这个新的随机端口上)，并且kube-proxy将开始接受该端口上的连接。
*   当一个客户端连接到该Service的虚拟IP地址时，iptables的规则被触发，并且将该网络报文重定向到kube-proxy自己的随机端口上，kube-proxy接收到请求之后，选择一个后端Pod，再将请求以代理的方式转发到后端Pod.
*   这意味着Service可以选择任意端口，而无需担心端口冲突。客户端可以直接连接到一个IP:Port,无需关心最终由哪个Pod提供服务。

##### 虚拟IP的实现方式之iptables（K8S默认使用）

*   ![](10.Kubernetes%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0.resources/B11FA8DD-EA7C-4476-99D0-653FEA767D5D.png)
    1.  kube-proxy 监听 kubernetes master 以获得添加和移除 Service / Endpoint 的事件
    2.  kube-proxy 在其所在的节点（每个节点都有 kube-proxy）上为每一个 Service 安装 iptable 规则
    3.  iptables 将发送到 Service 的 ClusterIP / Port 的请求重定向到 Service 的后端 Pod 上
        *   对于 Service 中的每一个 Endpoint，kube-proxy 安装一个 iptable 规则
        *   默认情况下，kube-proxy 随机选择一个 Service 的后端 Pod
*   当后端Service被创建时，kubernetes master会为其分配一个虚拟IP地址(假设为10.0.0.1),并假设Service的端口是1234.集群中所有的kube\_proxy都实时监听着Service的创建和删除。Service创建之后，kube-proxy设定了一系列的iptables规则(这些规则可将虚拟IP映射到per-Service的规则)。per-Service规则进一步连接到per-Endpoint规则，最终将网络请求重定向到后端Pod
*   当一个客户端连接到该Service的虚拟IP的时候，iptables的规则被触发。一个后端Pod将会被选中(基于session affinity或者随机选择)，且网络报文被重定向到该后端Pod，与userspace proxy不同，网络报文不在被复制到userspace，kube-proxy也无需处理这些报文，且报文被直接转发到后端。
    *   在使用node-port或load-balancer类型的Service时，以上的代理处理过程是一样的。

###### 优点

1.  更低的系统开销：在 linux netfilter 处理请求，无需在 userspace 和 kernel space 之间切换
2.  更稳定

###### 缺点

1.  使用 iptables mode 时，如果第一个 Pod 没有响应，则创建连接失败
2.  使用 user space mode 时，如果第一个 Pod 没有响应，kube-proxy 会自动尝试连接另外一个后端 Pod

* * *

##### 虚拟IP的实现方式之IPVS

*   ![](10.Kubernetes%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0.resources/B65BDE20-2ED3-49BB-91F7-E417A36E4874.png)
    *   kube-proxy 监听 kubernetes master 以获得添加和移除 Service / Endpoint 的事件
    *   kube-proxy 根据监听到的事件，调用 netlink 接口，创建 IPVS 规则；并且将 Service/Endpoint 的变化同步到 IPVS 规则中。当访问一个 Service 时，IPVS 将请求重定向到后端 Pod
*   在一个大型集群中（例如，存在 10000 个 Service）iptables 的操作将显著变慢。IPVS 的设计是基于 in-kernel hash table 执行负载均衡。因此，使用 IPVS 的 kube-proxy 在 Service 数量较多的情况下仍然能够保持好的性能。同时，基于 IPVS 的 kube-proxy 可以使用更复杂的负载均衡算法（最少连接数、基于地址的、基于权重的等）

###### 优点

*   PVS proxy mode 基于 netfilter 的 hook 功能，与 iptables 代理模式相似，但是 IPVS 代理模式使用 hash table 作为底层的数据结构，并在 kernel space 运作。这就意味着
    *   IPVS 代理模式可以比 iptables 代理模式有更低的网络延迟，在同步代理规则时，也有更高的效率
    *   与 user space 代理模式 / iptables 代理模式相比，IPVS 模式可以支持更大的网络流量
